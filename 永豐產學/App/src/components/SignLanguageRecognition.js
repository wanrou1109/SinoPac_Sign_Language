import React, { useState, useEffect, useRef } from 'react';
import * as handsModule from "@mediapipe/hands";
import { drawConnectors, drawLandmarks } from "@mediapipe/drawing_utils";
import { Camera } from "@mediapipe/camera_utils";
import { useNavigate, useLocation } from 'react-router-dom';
import { useAppContext } from '../contexts/AppContext.js';
import Header from './Header.js';
import '../styles/SignLanguageRecognition.css';

const SignLanguageRecognition = () => {
    const navigate = useNavigate();
    const location = useLocation();
    const { addMessage, editMessage, setRecognitionStatus, recognitionStatus, setConversations } = useAppContext();
    const [isRecording, setIsRecording] = useState(false);
    const [result, setResult] = useState('');
    const [isProcessing, setIsProcessing] = useState(false);
    const [isWaitingForMovement, setIsWaitingForMovement] = useState(false); // ç­‰å¾…å‹•ä½œçš„ flag

    const videoRef = useRef(null);
    const canvasRef = useRef(null);
    const mediaRecorderRef = useRef(null);
    const recordedChunksRef = useRef([]);
    const streamRef = useRef(null);
    const recognitionIntervalRef = useRef(null);
    const lastRecognizedTextRef = useRef('');
    const resultBoxRef = useRef(null);
    const keypointsBuffer = useRef([]);
    const finalResultRef = useRef('');
    const lastFrameRef = useRef(null); // å„²å­˜ä¸Šä¸€å¹€è³‡æ–™
    const editMessageID = location.state?.messageID;

    useEffect(() => {
        const setupCamera = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                streamRef.current = stream;
                if (videoRef.current) {
                    videoRef.current.srcObject = stream;
                }

                const hands = new handsModule.Hands({
                    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
                });

                hands.setOptions({
                    maxNumHands: 2,
                    modelComplexity: 1,
                    minDetectionConfidence: 0.7,
                    minTrackingConfidence: 0.5,
                });

                hands.onResults((results) => {
                    const canvasCtx = canvasRef.current.getContext('2d');
                    canvasCtx.clearRect(0, 0, 640, 480);
                    canvasCtx.save();
                    canvasCtx.translate(640, 0);
                    canvasCtx.scale(-1, 1);
                    if (videoRef.current) {
                        canvasCtx.drawImage(videoRef.current, 0, 0, 640, 480);
                    }

                    if (results.multiHandLandmarks) {
                        for (let i = 0; i < results.multiHandLandmarks.length; i++) {
                            const landmarks = results.multiHandLandmarks[i];
                            let handedness = results.multiHandedness?.[i]?.label || '';
                            const flippedLandmarks = landmarks.map(pt => ({
                                ...pt,
                                x: 1.0 - pt.x
                            }));
                            const cx = flippedLandmarks[0].x * 640;
                            const cy = flippedLandmarks[0].y * 480;

                            drawConnectors(canvasCtx, flippedLandmarks, handsModule.HAND_CONNECTIONS, { color: '#00FF00', lineWidth: 2 });
                            drawLandmarks(canvasCtx, flippedLandmarks, { color: '#FF0000', lineWidth: 1 });

                            canvasCtx.font = "16px Arial";
                            canvasCtx.fillStyle = "blue";
                            canvasCtx.save();
                            canvasCtx.scale(-1, 1);
                            canvasCtx.fillText(handedness, -cx, cy - 10);
                            canvasCtx.restore();
                        }
                    }
                    canvasCtx.restore();

                    if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
                        let leftHand = null;
                        let rightHand = null;

                        for (let i = 0; i < results.multiHandLandmarks.length; i++) {
                            const handLabel = results.multiHandedness?.[i]?.label;
                            if (handLabel === 'Left') leftHand = results.multiHandLandmarks[i];
                            else if (handLabel === 'Right') rightHand = results.multiHandLandmarks[i];
                        }

                        const leftFlat = leftHand ? leftHand.flatMap(pt => [pt.x, pt.y, pt.z]) : new Array(63).fill(0);
                        const rightFlat = rightHand ? rightHand.flatMap(pt => [pt.x, pt.y, pt.z]) : new Array(63).fill(0);
                        const combined = [...leftFlat, ...rightFlat];

                        keypointsBuffer.current.push(combined);
                        if (keypointsBuffer.current.length > 30) {
                            keypointsBuffer.current.shift();
                        }

                        window.keypointsArray = [...keypointsBuffer.current];

                        if (isWaitingForMovement) {
                            const last = lastFrameRef.current;
                            if (last) {
                                const threshold = 0.015;
                                const diff = Math.sqrt(
                                    combined.reduce((acc, val, i) => acc + Math.pow(val - last[i], 2), 0) / combined.length
                                );

                                if (diff > threshold) {
                                    console.log(`âœ… åµæ¸¬åˆ°ä½¿ç”¨è€…å‹•ä½œï¼ˆå·®ç•°é‡ ${diff.toFixed(4)}ï¼‰ â†’ æ¢å¾©è¾¨è­˜`);
                                    setIsWaitingForMovement(false);
                                }
                            }
                            lastFrameRef.current = combined;
                        }
                    } else {
                        keypointsBuffer.current = [];
                        window.keypointsArray = [];
                    }
                });

                const camera = new Camera(videoRef.current, {
                    onFrame: async () => {
                        if (!videoRef.current || videoRef.current.readyState < 2) return;
                        const flippedCanvas = document.createElement('canvas');
                        flippedCanvas.width = 640;
                        flippedCanvas.height = 480;
                        const flippedCtx = flippedCanvas.getContext('2d');
                        flippedCtx.translate(640, 0);
                        flippedCtx.scale(-1, 1);
                        flippedCtx.drawImage(videoRef.current, 0, 0, 640, 480);
                        await hands.send({ image: flippedCanvas });
                    },
                    width: 640,
                    height: 480,
                });
                camera.start();
            } catch (error) {
                console.error('é¡é ­é–‹å•Ÿå¤±æ•—:', error);
                alert('ç„¡æ³•é–‹å•Ÿé¡é ­ï¼Œè«‹ç¢ºèªå·²æˆäºˆæ”å½±æ©Ÿæ¬Šé™ã€‚');
            }
        };

        setupCamera();
        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
            if (recognitionIntervalRef.current) {
                clearInterval(recognitionIntervalRef.current);
                recognitionIntervalRef.current = null;
            }
        };
    }, []);

    useEffect(() => {
        fetch('/api/test')
            .then(res => res.json())
            .then(data => console.log('å¾Œç«¯é€£ç·šæˆåŠŸ:', data))
            .catch(err => console.error('é€£ç·šéŒ¯èª¤:', err));
    }, []);

    const startPeriodicRecognition = () => {
        console.log('ğŸ¯ å•Ÿå‹•å®šæœŸè¾¨è­˜');
        recognitionIntervalRef.current = setInterval(() => {
            if (!isProcessing && !isWaitingForMovement) {
                captureAndRecognize();
            }
        }, 2000);
    };

    const captureAndRecognize = async () => {
        if (!videoRef.current || !streamRef.current) return;
        try {
            setIsProcessing(true);
            setTimeout(() => {
                sendFrameToRecognition();
            }, 0);
        } catch (err) {
            console.error('éŒ¯èª¤:', err);
        } finally {
            setIsProcessing(false);
        }
    };

    const sendFrameToRecognition = async () => {
        try {
            const keypointsArray = window.keypointsArray;
            if (!keypointsArray || keypointsArray.length !== 30) return;
            if (keypointsArray.reduce((sum, row) => sum + row.length, 0) !== 30 * 126) return;

            const response = await fetch('/api/sign-language-recognition/frame', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ keypoints: keypointsArray }),
            });

            const data = await response.json();
            console.log('è¾¨è­˜çµæœï¼š', data);

            if (data.success === true && data.text?.trim()) {
                setIsWaitingForMovement(true); // âœ… åœæ­¢å¾ŒçºŒè¾¨è­˜ç›´åˆ°ä½¿ç”¨è€…å‹•ä½œ

                if (data.text === 'è¼¸å…¥å®Œæˆ' && data.raw_label) {
                    setResult(prev => {
                        const newResult = prev + '\n' + data.raw_label;
                        finalResultRef.current = newResult;
                        return newResult;
                    });
                    return;
                }

                const isDigit = /^\d$/.test(data.text);
                if (isDigit || data.text !== lastRecognizedTextRef.current) {
                    lastRecognizedTextRef.current = data.text;
                    setResult(prev => {
                        const newResult = prev + data.text;
                        finalResultRef.current = newResult;
                        return newResult;
                    });
                }
            }
        } catch (err) {
            console.error('ç™¼é€è¾¨è­˜éŒ¯èª¤:', err);
        }
    };

    const handleStartRecording = () => {
        if (!streamRef.current) {
            alert('é¡é ­å°šæœªæº–å‚™å°±ç·’');
            return;
        }

        setResult('');
        lastRecognizedTextRef.current = '';
        finalResultRef.current = '';
        recordedChunksRef.current = [];

        const mediaRecorder = new MediaRecorder(streamRef.current, { mimeType: 'video/webm' });
        mediaRecorder.ondataavailable = event => {
            if (event.data.size > 0) {
                recordedChunksRef.current.push(event.data);
            }
        };
        mediaRecorder.onstop = async () => {
            clearInterval(recognitionIntervalRef.current);
            recognitionIntervalRef.current = null;
            try {
                const blob = new Blob(recordedChunksRef.current, { type: 'video/webm' });
                setRecognitionStatus('idle');
                const finalResult = finalResultRef.current || 'ç„¡è¾¨è­˜çµæœ';
                if (editMessageID) editMessage(editMessageID, finalResult);
                else addMessage(finalResult, 'customer');
                navigate('/conversation');
            } catch (err) {
                alert('è¾¨è­˜å¤±æ•—: ' + err.message);
                setRecognitionStatus('idle');
            }
        };

        mediaRecorderRef.current = mediaRecorder;
        mediaRecorder.start(1000);
        setIsRecording(true);
        setRecognitionStatus('recording');
        startPeriodicRecognition();
    };

    const handleStopRecording = () => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
            mediaRecorderRef.current.stop();
        }
        clearInterval(recognitionIntervalRef.current);
        recognitionIntervalRef.current = null;
        setIsRecording(false);
        setRecognitionStatus('processing');
    };

    const handleBack = () => {
        if (mediaRecorderRef.current && isRecording) mediaRecorderRef.current.stop();
        clearInterval(recognitionIntervalRef.current);
        recognitionIntervalRef.current = null;
        setIsRecording(false);
        setRecognitionStatus('idle');
        navigate('/conversation');
    };

    return (
        <div className='sign-language-recognition-screen'>
            <Header showBackButton={true} onBack={handleBack}/>
            <div className='recognition-container'>
                <div className='video-container'></div>
                <video ref={videoRef} autoPlay playsInline muted style={{ display: 'none' }} />
                <canvas ref={canvasRef} className="overlay-canvas" width={640} height={480} />
                {isProcessing && <div className='processing-indicator'>è¾¨è­˜ä¸­...</div>}
            </div>
            <div className={`recognition-result ${isRecording ? 'visible' : 'hidden'}`}>
                <h3>å³æ™‚è¾¨è­˜çµæœï¼š</h3>
                <div ref={resultBoxRef} className={`result-text ${result ? 'has-content' : ''}`}>
                    {result ? (
                        <div className="recognized-summary">
                            <div className="prompt">è«‹æ‰“ä¸‹ä¸€å€‹å­—</div>
                            <div><strong>ç›®å‰å·²è¼¸å…¥ï¼š</strong> {result.replace(/è«‹æ‰“ä¸‹ä¸€å€‹å­—/g, '').trim()}</div>
                        </div>
                    ) : 'ç­‰å¾…æ‰‹èªè¾¨è­˜...'}
                </div>
                {isRecording && <div className="live-indicator">å³æ™‚æ›´æ–°ä¸­</div>}
            </div>
            <div className='action-bar'>
                {!isRecording ? (
                    <button className='record-button' onClick={handleStartRecording} disabled={recognitionStatus === 'processing'}>
                        <div className='button-inner'></div>
                    </button>
                ) : (
                    <button className='record-button recording-active' onClick={handleStopRecording}>
                        <div className='button-inner'></div>
                    </button>
                )}
            </div>
        </div>
    );
};

export default SignLanguageRecognition;
