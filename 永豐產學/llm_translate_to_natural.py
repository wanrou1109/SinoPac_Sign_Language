import os
import re
import jieba
import faiss
import numpy as np
from docx import Document
from zhconv import convert
from langdetect import detect
from typing import Dict, List, Tuple
from dotenv import load_dotenv
from deep_translator import GoogleTranslator
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# === ğŸ” è¼‰å…¥ API é‡‘é‘°èˆ‡æ¨¡å‹è¨­å®š ===
load_dotenv(dotenv_path="App/.env")
api_key = os.getenv("OPENROUTER_API_KEY")
base_url = "https://openrouter.ai/api/v1"
client = OpenAI(api_key=api_key, base_url=base_url)

# === è¼‰å…¥èªæ–™åº«ä¸¦å»ºç«‹ FAISS å‘é‡ç´¢å¼• ===
corpus_path = "rag_sentence.docx"
embedder = SentenceTransformer("shibing624/text2vec-base-chinese")

def load_corpus_from_docx(file_path, max_len=80) -> List[str]:
    doc = Document(file_path)
    chunks = []
    for para in doc.paragraphs:
        text = para.text.strip()
        if not text:
            continue
        words = list(jieba.cut(text))
        cur = []
        for w in words:
            cur.append(w)
            if len("".join(cur)) >= max_len:
                chunks.append("".join(cur))
                cur = []
        if cur:
            chunks.append("".join(cur))
    return chunks

corpus = load_corpus_from_docx(corpus_path)
corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True)
dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(corpus_embeddings)

def retrieve_relevant_sentences(query: str, k=5) -> List[str]:
    q_emb = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(q_emb, k)
    # åªä¿ç•™ç›¸ä¼¼åº¦é«˜æ–¼é–€æª»çš„å¥å­
    threshold = 0.6
    filtered = [corpus[i] for d, i in zip(D[0], I[0]) if d < threshold]
    if not filtered:
        filtered = [query]

    return filtered


def ensure_traditional_chinese(text: str) -> Tuple[str, List[Dict[str, str]]]:
    translator = GoogleTranslator(source='auto', target='zh-TW')
    records = []
    words = text.split()
    result = []
    for word in words:
        try:
            if not re.match('^[\u4e00-\u9fff\sï¼Œã€‚ï¼ï¼Ÿã€]+$', word):
                lang = detect(word)
                if lang not in ('zh-tw', 'zh-cn'):
                    trans = translator.translate(word)
                    records.append({'original': word, 'translated': trans, 'detected_lang': lang})
                    word = trans
            word = convert(word, 'zh-hant')
        except Exception:
            pass
        result.append(word)
    return ' '.join(result), records

# === ä¾›å¤–éƒ¨ä½¿ç”¨çš„ä¸»å‡½æ•¸ ===
def translate_to_natural(user_message: str) -> str:
    # âœ… å°çŸ­å¥ä¸æª¢ç´¢ï¼Œç›´æ¥ç¿»è­¯
    if len(user_message.strip().split()) <= 1:
        related = []
    else:
        related = retrieve_relevant_sentences(user_message, k=5)

    context = "\n".join(f"- {s}" for s in related)

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‰‹èªç¿»è­¯å°ˆå®¶ï¼Œè«‹å°‡ã€Œæ‰‹èªèªåºã€è½‰æ›æˆè‡ªç„¶çš„ä¸­æ–‡èªå¥ï¼Œä¸¦æ¨¡ä»¿éŠ€è¡Œè¡Œå“¡çš„å£å»ã€‚
ä½ **åªèƒ½**ä½¿ç”¨**ç¹é«”ä¸­æ–‡**åŠä¸­æ–‡æ¨™é»ä½œç­”ï¼Œ**çµ•å°ä¸è¦**æ‘»ä»»ä½•è‹±æ–‡å­—æ¯ã€æ‹¼éŸ³æˆ–å…¶ä»–èªç¨®ç¬¦è™Ÿã€‚
**è«‹æ³¨æ„ï¼š**
1. åªè¼¸å‡ºè‡ªç„¶èªåºçš„ä¸­æ–‡å¥å­ï¼Œ**ä¸è¦**åŠ å…¥ä»»ä½•å¤šé¤˜èªªæ˜æ–‡å­—ã€‚
2. å¦‚æœè¼¸å…¥èˆ‡åŠ‡æœ¬ä¾‹å¥ç›¸ä¼¼ï¼Œè«‹ç›¡é‡å¥—ç”¨ç›¸åŒå¥å‹ã€‚
3. å‡è¨­ä½ æ˜¯éŠ€è¡Œè¡Œå“¡ï¼Œç”¨ç¦®è²Œã€æ¸…æ™°çš„å®¢æœèªæ°£å›ç­”ã€‚

**ç¯„ä¾‹å°ç…§ï¼š**
- æ‰‹èªï¼šæˆ‘ ç”³è«‹ å­˜æ‘º  
  è½‰è­¯ï¼šæˆ‘æƒ³è¦è¾¦ç†é–‹æˆ¶ã€‚
- æ‰‹èªï¼šå­˜éŒ¢ ç”¨ç”¨ æƒ³ ç”³è«‹ å­˜æ‘º å°é¢åç¨± å„å¼ 
  è½‰è­¯ï¼šæˆ‘è¦ç”¨æ–¼å­˜æ¬¾ï¼Œæƒ³é–‹ç¶œåˆå­˜æ¬¾å¸³æˆ¶ã€‚
- æ‰‹èªï¼šç´™ ç°½å å®Œäº†
  è½‰è­¯ï¼š æˆ‘å·²ç¶“åœ¨ä¸Šé¢ç°½åäº†ã€‚ 



**åŠ‡æœ¬èªæ–™ï¼ˆåƒè€ƒï¼‰ï¼š**
{context}

---
è«‹å°‡ä¸‹åˆ—æ‰‹èªèªåºï¼Œè½‰æ›ç‚ºè‡ªç„¶çš„ä¸­æ–‡å¥å­ï¼š  
{user_message}
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

    resp = client.chat.completions.create(
        model="qwen/qwen2.5-vl-72b-instruct",
        messages=messages
    )
    bot_reply = resp.choices[0].message.content.strip()
    converted_reply, _ = ensure_traditional_chinese(bot_reply)
    return converted_reply

llm_translate_to_natural = translate_to_natural